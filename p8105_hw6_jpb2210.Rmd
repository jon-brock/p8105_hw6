---
title: "Data Science - Homework #6"
author: "Jon Brock - JPB2210"
output: 
    github_document:
        toc: TRUE
        toc_depth: 2
    
---

```{r load packages, message = FALSE}
library(tidyverse)
library(modelr)
```

***

## Problem #1
###### (*45 points*)
##### *Working with linear models*  

I reviewed each vector to determine what vector type each one should be when imported. There is a majority of integer values, so the `.default` value for non-specified vectors applies as such. I specified `babysex`, `frace`, `malform`, and `mrace` as `<fct>` vectors; `gaweeks`, `ppbmi`, and `smoken` are specified as `<dbl>` vectors. I will further tidy--relevel and relabel--my specified `<fct>` vectors in subsequent code.  

```{r load and tidy data}
bw_df <- read_csv("./data/birthweight.csv",
                  col_names = TRUE,
                  col_types = cols(
                      .default = col_integer(),
                      babysex = col_factor(),
                      frace = col_factor(),
                      gaweeks = col_double(),
                      malform = col_factor(),
                      mrace = col_factor(),
                      ppbmi = col_double(),
                      smoken = col_double()
                      )
                  )

bw_df
```


It is rather tedious to check for `NA` values within individual vectors one at a time. So, instead, I decided to check all vectors at once by using a "soft-deprecated" `dplyr` function (`funs()`) to summarize all of the `NA` observations within each vector in my data frame.  

```{r checking all columns for na values, warning = FALSE}
options(tibble.width = Inf)

bw_df %>% 
    select(everything()) %>% 
    summarize_all(funs(sum(is.na(.))))
```

The output shows that we have `0` observations of `NA` in all of our vectors. We're good to go!  

```{r variable renaming and recoding}
bw_df <- 
    bw_df %>% 
    mutate(
        baby_sex = recode(babysex, "1" = "male", "2" = "female"),
        baby_head_cm = bhead,
        baby_length_cm = blength,
        baby_birthweight_grams = bwt,
        mother_weight_grams = delwt * 453.492,
        family_income = fincome,
        father_race = recode(frace, "1" = "white", "2" = "black", "3" = "asian", "4" = "p_rican", "8" = "other", "9" = "unknown"),
        gestation_age_wks = gaweeks,
        malformations_present = recode(malform, "0" = "absent", "1" = "present"),
        mother_menarche_age = menarche,
        mother_height_cm = mheight * 2.54,
        mother_delivery_age = momage,
        mother_race = recode(mrace, "1" = "white", "2" = "black", "3" = "asian", "4" = "p_rican", "8" = "other"),
        number_prior_births = parity,
        number_prior_low_bwt_babies = pnumlbw,
        number_prior_small_ga_babies = pnumsga,
        mother_pre_pregnancy_bmi = ppbmi,
        mother_pre_pregnancy_weight_grams = ppwt * 453.492,
        avg_number_cigs_per_day = smoken,
        mother_pregnancy_weight_gain_grams = wtgain * 453.492) %>% 
    select(c(baby_sex:mother_pregnancy_weight_gain_grams))

colnames(bw_df)
```

We have successfully recoded and renamed all `20` of our variables, and dropped the original `20` for lack of further need.  It's important to note that variable  measurements were incongruent in terms of scale -- both metric and imperial were used. To correct for this I made the necessary conversions so that all measurements are metric (i.e., weight = grams, height/length = centimeters).  

```{r fitting a linear model}
jon_fit <- lm(baby_birthweight_grams ~ 
                  gestation_age_wks +
                  mother_weight_grams +
                  number_prior_low_bwt_babies +
                  avg_number_cigs_per_day,
              data = bw_df)

broom::tidy(jon_fit) %>% 
    knitr::kable()
```

```{r plotting of model, predictions, and residuals, warning = FALSE, fig.align = "center"}
bw_df %>% 
    add_predictions(jon_fit) %>% 
    add_residuals(jon_fit) %>% 
    ggplot(aes(x = gestation_age_wks, y = baby_birthweight_grams)) + 
        geom_point() + 
        stat_smooth(method = "lm")
        #geom_line(aes(y = pred), color = "red")
```

***  

## Problem #2
###### (*35 points*)
##### *Bootstrapping*  

This problem focuses on bootstrapping. First, we import a dataset from NOAA that contains the `minimum` and `maximum` temperatures for Central Park, NY from the time period 2017-01-10 through 2017-12-31.

```{r pre-specified data import code, message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df
```

Now we will generate 5000 bootstrap samples from the `weather_df` data frame, and fit a simple linear regression with `tmax` as the response and `tmin` as the predictor for each of the 5000 samples. We only want to look at the distribution of the R-squared value, as well as the log of the product of Beta0 and Beta1, so we will remove all other items in the resulting data frame.

```{r bootstrap linear models, warning = FALSE}
set.seed(90210) #See what I did there?

boot_strap <- 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    tidy_models = map(models, broom::tidy),
    glance_models = map(models, broom::glance)) %>% 
  unnest(tidy_models, glance_models) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate) %>% 
  rename(
    beta_0 = `(Intercept)`,
    beta_1 = tmin) %>% 
  mutate(log_b0xb1 = log(beta_0*beta_1)) %>% 
  select(r.squared, log_b0xb1)

boot_strap
```

We now have our bootstrap samples of R-squared values and logs of the product of Beta0 and Beta1. Now we can generate a density plot for the values and products.

```{r density plot of r.squared values, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
boot_strap %>% 
    ggplot(aes(x = r.squared)) + 
    geom_density(fill = "lightgreen", show.legend = FALSE) +
    labs(
        title = "Distribution of" ~R^2,
        subtitle = "Based on 5000 bootstrap samples of a linear model (tmax ~ tmin)",
        x = quote(R^2)
    )
```

We can see from the R-squared density plot that there is a high density of R-squared values crowded around a center of R = ~0.91. As we recall from our introductory stats course(s), an R-Squared value falls between 0 and 1, and tells us how much of the variation in `y` can be explained by `x`. In this case, a vast majority of the variation in `tmax` is explained by `tmin`.  

The 95% confidence interval of `R-squared` is (`r round(quantile(pull(boot_strap, r.squared), probs = c(0.025,0.975)), digits = 2)`).

```{r density plot of log(beta0*beta1), fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
boot_strap %>% 
    ggplot(aes(x = log_b0xb1)) +
    geom_density(fill = "cornflowerblue", show.legend = FALSE) +
    labs(
        title = "Distribution of" ~log(hat(beta)[0] %*% hat(beta)[1]),
        subtitle = "Based on 5000 bootstrap samples of a linear model (tmax ~ tmin)",
        x = quote(log(hat(beta)[0] %*% hat(beta)[1]))
    )
```

As for the density plot of $$log(\beta_1*\beta_0)$$, we see that the distribution is normal, and centered around a $$log(\beta_1*\beta_0)$$ value of ~2.025.

The 95% confidence interval of $$log(\beta_1*\beta_0)$$ is (`r round(quantile(pull(boot_strap, log_b0xb1), probs = c(0.025,0.975)), digits = 2)`).  

***  

## Return of the Bonus Content  

<center> ![](bonus_graph.png) </center>  
**h/t:[Toward Data Science](https://towardsdatascience.com/10-tips-to-improve-your-plotting-f346fa468d18)**