---
title: "Data Science - Homework #6"
author: "Jon Brock - JPB2210"
output: 
    github_document:
        toc: TRUE
        toc_depth: 2
---

```{r load packages, message = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
```

***

## Problem #1
###### (*45 points*)
##### *Working with linear models*  

I reviewed each vector to determine what vector type each one should be when imported. There is a majority of integer values, so the `.default` value for non-specified vectors applies as such. I specified `babysex`, `frace`, `malform`, and `mrace` as `<fct>` vectors; `gaweeks`, `ppbmi`, and `smoken` are specified as `<dbl>` vectors. I will further tidy--relevel and relabel--my specified `<fct>` vectors in subsequent code.  

```{r load and tidy data}
bw_df <- read_csv("./data/birthweight.csv",
                  col_names = TRUE,
                  col_types = cols(
                      .default = col_integer(),
                      babysex = col_factor(),
                      frace = col_factor(),
                      gaweeks = col_double(),
                      malform = col_factor(),
                      mrace = col_factor(),
                      ppbmi = col_double(),
                      smoken = col_double()
                      )
                  )

bw_df
```

It is rather tedious to check for `NA` values within individual vectors one at a time. So, instead, I decided to check all vectors at once by using a "soft-deprecated" `dplyr` function (`funs()`) to summarize all of the `NA` observations within each vector in my data frame.  

```{r checking all columns for na values, warning = FALSE}
options(tibble.width = Inf)

bw_df %>% 
    select(everything()) %>% 
    summarize_all(funs(sum(is.na(.))))
```

The output shows that we have `0` observations of `NA` in all of our vectors. We're good to go!  

```{r variable renaming and recoding}
bw_df <- 
    bw_df %>% 
    mutate(
        baby_sex = recode(babysex, "1" = "male", "2" = "female"),
        baby_head_cm = bhead,
        baby_length_cm = blength,
        baby_birthweight_grams = bwt,
        mother_weight_grams = delwt * 453.492,
        family_income = fincome,
        father_race = recode(frace, "1" = "white", "2" = "black", "3" = "asian", "4" = "p_rican", "8" = "other", "9" = "unknown"),
        gestation_age_wks = gaweeks,
        malformations_present = recode(malform, "0" = "absent", "1" = "present"),
        mother_menarche_age = menarche,
        mother_height_cm = mheight * 2.54,
        mother_delivery_age = momage,
        mother_race = recode(mrace, "1" = "white", "2" = "black", "3" = "asian", "4" = "p_rican", "8" = "other"),
        number_prior_births = parity,
        number_prior_low_bwt_babies = pnumlbw,
        number_prior_small_ga_babies = pnumsga,
        mother_pre_pregnancy_bmi = ppbmi,
        mother_pre_pregnancy_weight_grams = ppwt * 453.492,
        avg_number_cigs_per_day = smoken,
        mother_pregnancy_weight_gain_grams = wtgain * 453.492) %>% 
    select(c(baby_sex:mother_pregnancy_weight_gain_grams))

# I am only printing the column names because the data output is excessive due to column name size
colnames(bw_df)
```

We have successfully recoded and renamed all `20` of our variables, and dropped the original `20` for lack of further need. It's important to note that variable  measurements were incongruent in terms of scale -- both metric and imperial were used. To correct for this I made the necessary conversions so that all measurements are metric (i.e., weight = grams, height/length = centimeters).  

***

Now let's fit a linear model with hypothesized predictor variables. Based on all of the previous CUMC Epidemiology courses' content, I believe that gestational age (`gestation_age_wks`), mother's weight (`mother_weight_grams`), number of prior low-birthweight babies (`number_prior_low_bwt_babies`), and average number of cigarettes smoked per day (`avg_number_cigs_per_day`) are strong predictors of the birthweight of a baby. Subsequently, we fit our linear model as such:  

```{r fitting a self-specified linear model}
jon_fit <- lm(baby_birthweight_grams ~ 
                  gestation_age_wks +
                  mother_weight_grams +
                  number_prior_low_bwt_babies +
                  avg_number_cigs_per_day,
              data = bw_df)

broom::tidy(jon_fit) %>% 
    knitr::kable()
```

Now, let's plot our `jon_fit` linear model's predicted values against its residuals to see how well our model fits the data.  

```{r fitting residuals against predictions, warning = FALSE, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
predicted_vs_residuals_jon_fit <- 
    bw_df %>% 
    add_predictions(jon_fit) %>% 
    add_residuals(jon_fit) %>% 
    select(pred, resid)

predicted_vs_residuals_jon_fit  %>% 
    ggplot(aes(x = pred, y = resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", size = 1, linetype = 2) +
    labs(
        title = "Versus Fits of jon_fit Linear Model",
        x = "Fitted Value (grams)",
        y = "Residual (grams)")
```

We can see that there is a large variance within the specified model, as many points are far from the (dotted) prediction line. Let's now compare our specified model to two additional models:  

1. p8105_fit_1 = `baby_length_cm + gestation_age_wks` (main effects only)  
1. p8105_fit_2 = `baby_head_cm * baby_length_cm * baby_sex` (main effects plus interactions between all predictors)  

```{r fitting a requested linear model (A)}
p8105_fit_1 <- lm(baby_birthweight_grams ~ baby_length_cm + gestation_age_wks, data = bw_df)

broom::tidy(p8105_fit_1) %>% 
    knitr::kable()
``` 

```{r fitting a requested linear model (B)}
p8105_fit_2 <- lm(baby_birthweight_grams ~ baby_head_cm * baby_length_cm * baby_sex, data = bw_df)

broom::tidy(p8105_fit_2) %>% 
    knitr::kable()
``` 

I don't believe this is required (for this problem), but we can also plot these two models' predicted values against their corresponding residuals to see how well their models fit their data.  

```{r fitting residuals against predictions (A), warning = FALSE, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
predicted_vs_residuals_p8015_fit_1 <- 
    bw_df %>% 
    add_predictions(p8105_fit_1) %>% 
    add_residuals(p8105_fit_1) %>% 
    select(pred, resid)

predicted_vs_residuals_p8015_fit_1 %>% 
    ggplot(aes(x = pred, y = resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", size = 1, linetype = 2) +
    labs(
        title = "Versus Fits of p8105_fit_1 Linear Model",
        x = "Fitted Value (grams)",
        y = "Residual (grams)")
```

```{r fitting residuals against predictions (B), warning = FALSE, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
predicted_vs_residuals_p8015_fit_2 <- 
    bw_df %>% 
    add_predictions(p8105_fit_2) %>% 
    add_residuals(p8105_fit_2) %>% 
    select(pred, resid)

predicted_vs_residuals_p8015_fit_2 %>% 
    ggplot(aes(x = pred, y = resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", size = 1, linetype = 2) +
    labs(
        title = "Versus Fits of p8105_fit_2 Linear Model",
        x = "Fitted Value (grams)",
        y = "Residual (grams)")
```

Based on those plots, it appears as though the second model (`p8105_fit_1`) has the least error variance. But let us use cross validation methods in order to check all of our models for the best fit. We start by producing 500 training/testing data splits of our original data `bw_df`. [Note: I chose `500` as the `n` because I was not sure what number to select. It appeared to be a user-specified value.]  

```{r generate test/train datasets}
cv_df <-
    crossv_mc(bw_df, 500)
```

Now, we will generate fitted models for each of the three linear models specified, as well as generate the root mean squared error `RMSE` for each of the fitted models.  

```{r generate rmse values for iterated linear models, warning = FALSE}
cv_df <- 
    cv_df %>% 
    mutate(jon_model = map(train, ~jon_fit, data = .x),
           p8105_1_model = map(train, ~p8105_fit_1, data = .x),
           p8105_2_model = map(train, ~p8105_fit_2, data = .x)) %>% 
    mutate(rmse_jon = map2_dbl(jon_model, test, ~rmse(model = .x, data = .y)),
           rmse_p8105_1 = map2_dbl(p8105_1_model, test, ~rmse(model = .x, data =.y)),
           rmse_p8105_2 = map2_dbl(p8105_2_model, test, ~rmse(model = .x, data = .y)))
```

We can now plot out results to visually inspect which model has the best fitting.  

```{r generate plot of model comparisons, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
    geom_violin(fill = "tan1") +
    labs(
        title = "Cross Validation of Specified Models",
        x = "Model",
        y = "Root Mean Squared Error (RMSE)")
```

Even though our predicted vs. residuals plots indicated that `p8105_1` had the least amount of error variance, we see from our cross validation plot that the `p8105_2` model has the best `RMSE` and is the best fit. Recall that this model has both our main effects, as well as the interaction between all three predictors, included.  

***  

## Problem #2
###### (*35 points*)
##### *Bootstrapping*  

This problem focuses on bootstrapping. First, we import a dataset from NOAA that contains the `minimum` and `maximum` temperatures for Central Park, NY from the time period 2017-01-10 through 2017-12-31.

```{r pre-specified data import code, message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df
```

Now we will generate 5000 bootstrap samples from the `weather_df` data frame, and fit a simple linear regression with `tmax` as the response and `tmin` as the predictor for each of the 5000 samples. We only want to look at the distribution of the R-squared value, as well as the log of the product of Beta0 and Beta1, so we will remove all other items in the resulting data frame.

```{r bootstrap linear models, warning = FALSE}
set.seed(90210) #See what I did there?

boot_strap <- 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    tidy_models = map(models, broom::tidy),
    glance_models = map(models, broom::glance)) %>% 
  unnest(tidy_models, glance_models) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate) %>% 
  rename(
    beta_0 = `(Intercept)`,
    beta_1 = tmin) %>% 
  mutate(log_b0xb1 = log(beta_0*beta_1)) %>% 
  select(r.squared, log_b0xb1)

boot_strap
```

We now have our bootstrap samples of R-squared values and logs of the product of Beta0 and Beta1. Now we can generate a density plot for the values and products.

```{r density plot of r.squared values, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
boot_strap %>% 
    ggplot(aes(x = r.squared)) + 
    geom_density(fill = "lightgreen", show.legend = FALSE) +
    geom_vline(xintercept = 0.913, color = "red", size = 1, linetype = 2) +
    labs(
        title = "Distribution of" ~R^2,
        subtitle = "Based on 5000 bootstrap samples of a linear model (tmax ~ tmin)",
        x = quote(R^2)
    )
```

We can see from the R-squared density plot that there is a high density of R-squared values crowded around a center of R = ~0.913. As we recall from our introductory stats course(s), an R-Squared value falls between 0 and 1, and tells us how much of the variation in `y` can be explained by `x`. In this case, a vast majority of the variation in `tmax` is explained by `tmin`.  

The 95% confidence interval of `R-squared` is (`r round(quantile(pull(boot_strap, r.squared), probs = c(0.025,0.975)), digits = 2)`).

```{r density plot of log(beta0*beta1), fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
boot_strap %>% 
    ggplot(aes(x = log_b0xb1)) +
    geom_density(fill = "cornflowerblue", show.legend = FALSE) +
    geom_vline(xintercept = 2.012, color = "orange4", size = 1, linetype = 2) +
    labs(
        title = "Distribution of" ~log(hat(beta)[0] %*% hat(beta)[1]),
        subtitle = "Based on 5000 bootstrap samples of a linear model (tmax ~ tmin)",
        x = quote(log(hat(beta)[0] %*% hat(beta)[1]))
    )
```

As for the density plot of `log(beta_0 x beta_1)`, we see that the distribution is normal, and centered around a `log(beta_0 x beta_1)` value of ~2.012.

The 95% confidence interval of `log(beta_0 x beta_1)` is (`r round(quantile(pull(boot_strap, log_b0xb1), probs = c(0.025,0.975)), digits = 2)`).  

***  

## Return of the Bonus Content  

<center> ![](bonus_graph.png) </center>  
**h/t:[Toward Data Science](https://towardsdatascience.com/10-tips-to-improve-your-plotting-f346fa468d18)**